@inproceedings{topstatprop,
 author = {Nadjahi, Kimia and Durmus, Alain and Chizat, L\'{e}na\"{\i}c and Kolouri, Soheil and Shahrampour, Shahin and Simsekli, Umut},
 booktitle = {NeurIPS},
 title = {Statistical and Topological Properties of Sliced Probability Divergences},
 year = {2020}
}
@inproceedings{slicedmultimar,
 author = {Cohen, Samuel and Kumar, K S Sesh and Deisenroth, Marc},
 booktitle = {In submission},
 title = {Sliced Multi-Marginal Optimal Transport},
 year = {2021}
}

@article{
mw_properties,
title={Multi-Marginal Optimal Transport Defines a Generalized Metric},
author={Jose Bento and Liang Mi},
journal={arXiv:2001.11114},
year={2020}
}


@article{
altschulernpbary,
title={Wasserstein Barycenters are NP-hard to Compute},
author={Jason Altschuler and Enric Boix-Adsera},
journal={arXiv:2101.01100},
year={2021}
}
@article{distribshift,
title	= {Concrete Problems in AI Safety},
author	= {Dario Amodei and Chris Olah and Jacob Steinhardt and Paul Christiano and John Schulman and Dan Mané},
year	= {2016},
journal	= {arXiv:1606.06565}
}



@article{accmmot,
  title={Multimarginal Optimal Transport by Accelerated Alternating Minimization},
  author={N. Tupitsa and P. Dvurechensky and A. Gasnikov and C{\'e}sar A. Uribe},
  journal={CDC},
  year={2020},
  pages={6132-6137}
}

@article{
mw_compl,
title={On the Complexity of Approximating Multimarginal Optimal Transport},
author={Tianyi Lin and Nhat Ho and Marco Cuturi and Michael I. Jordan},
journal = {arXiv:1910.00152},
year={2019}
}

@article{Davis:2018,
  Author = {Davis, Damek and Drusvyatskiy, Dmitriy},
  Journal = {arXiv:1802.02988},
  Title = {Stochastic Subgradient Method Converges at the Rate $\mathcal{O}(k^{-1/4})$ on Weakly Convex Functions},
  Year = {2018},
  
}



@article{article_anderes,
author = {Anderes, Ethan and Borgwardt, Steffen and Miller, Jacob},
year = {2015},
title = {Discrete {W}asserstein Barycenters: Optimal Transport for Discrete Data},
journal = {Mathematical Methods of Operations Research},
volume = {84},
pages = {}
}

@inproceedings{Chu:2019,
  Abstract = {Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various...},
  Author = {Chu, Casey and Minami, Kentaro and Fukumizu, Kenji},
  Title = {Smoothness and {Stability} in {GANs}},
  Year = {2019},
  }



@InProceedings{Srivastava2015,
  author    = {Sanvesh Srivastava and Volkan Cevher and Quoc Dinh and David Dunson},
  title     = {{WASP:} Scalable {B}ayes via Barycenters of Subset Posteriors},
  booktitle = {AISTATS},
  year      = {2015}
}






@InProceedings{pmlr-v80-claici18a,
  title = 	 {Stochastic {W}asserstein Barycenters},
  author = 	 {Claici, Sebastian and Chien, Edward and Solomon, Justin},
  booktitle = 	 {ICML},
  year = 	 {2018}
}

@incollection{NIPS2018_8274,
title = {Decentralize and Randomize: Faster Algorithm for {W}asserstein Barycenters},
author = {Dvurechenskii, Pavel and Dvinskikh, Darina and Gasnikov, Alexander and Uribe, Cesar and Nedich, Angelia},
booktitle = {NeurIPS},
year = {2018}
}

@incollection{NIPS2017_6858,
title = {Parallel Streaming {W}asserstein Barycenters},
author = {Staib, Matthew and Claici, Sebastian and Solomon, Justin M and Jegelka, Stefanie},
booktitle = {NeurIPS},
year = {2017}
}



@InProceedings{pmlr-v32-cuturi14,
  title = 	 {Fast Computation of {W}asserstein Barycenters},
  author = 	 {Marco Cuturi and Arnaud Doucet},
  booktitle = 	 {ICML},
  year = 	 {2014},
  }
@article{MCCANN1997153,
title = "A Convexity Principle for Interacting Gases",
journal = "Advances in Mathematics",
author = "Robert J. McCann",
volume = "128",
number = "1",
pages = "153 - 179",
year = "1997"

}

@article{journals/siamma/AguehC11,
  author = {Agueh, Martial and Carlier, Guillaume},
  journal = {SIAM Journal on Mathematical Analysis},
  number = 2,
  pages = {904-924},
  title = {Barycenters in the Wasserstein Space.},
  volume = 43,
  year = 2011
}



@inproceedings{DBLP:conf/birthday/BottouALO17,
  author    = {L{\'{e}}on Bottou and
               Mart{\'{\i}}n Arjovsky and
               David Lopez{-}Paz and
               Maxime Oquab},

  title     = {Geometrical Insights for Implicit Generative Modeling},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer},
  year      = {2017},
}


@book{geninc,
 author = {Cohen,Samuel and Sejdinovic, Dino},
 title = {Learning Coupled Deep Generative Models},
 year = {2019}
} 




@InProceedings{pmlr-v84-genevay18a,
  title = 	 {Learning Generative Models with {S}inkhorn Divergences},
  author = 	 {Aude Genevay and Gabriel Peyre and Marco Cuturi},
  booktitle = 	 {AISTATS},
  year = 	 {2018}
}@inproceedings{10.5555/3305381.3305474,
author = {Cuturi, Marco and Blondel, Mathieu},
title = {Soft-DTW: A Differentiable Loss Function for Time-Series},
year = {2017},
publisher = {JMLR.org},
booktitle = {ICML},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML’17}
}

@article{bonneel,
  title={{Sliced and Radon Wasserstein Barycenters of Measures}},
  author={Bonneel, Nicolas and Rabin, Julien and Peyr{\'e}, Gabriel and Pfister, Hanspeter},
  journal={Journal of Mathematical Imaging and Vision},
  volume={51},
  number={1},
  pages={22--45},
  year={2015},
  publisher={Springer}
}

@inproceedings{Sinkhorn1974DiagonalET,
  title={Diagonal Equivalence to Matrices with Prescribed Row and Column Sums},
  author={Richard Sinkhorn},
  journal = {Proceedings of the American Mathematical Society},
 volume = {45},
 number = {2},
 pages = {195--198},
  year={1974}
}
@inproceedings{ijcai2019-483,
  title     = {Parallel Wasserstein Generative Adversarial Nets with Multiple Discriminators},
  author    = {Su, Yuxin and Zhao, Shenglin and Chen, Xixian and King, Irwin and Lyu, Michael},
  booktitle = {IJCAI},
  year      = {2019}
}

  @inproceedings{DBLP:conf/aistats/JanatiCG19,
  author    = {Hicham Janati and
               Marco Cuturi and
               Alexandre Gramfort},
  title     = {Wasserstein Regularization for Sparse Multi-Task Regression},
  booktitle = {AISTATS},
  year      = {2019},

}







@InProceedings{pmlr-v89-genevay19a,
  title = 	 {Sample Complexity of {S}inkhorn Divergences},
  author = 	 {Genevay, Aude and Chizat, L\'{e}na\"{i}c and Bach, Francis and Cuturi, Marco and Peyr\'{e}, Gabriel},
  booktitle = 	 {AISTATS},
  year={2019}
}

@inproceedings{DBLP:journals/corr/RadfordMC15,
  author    = {Alec Radford and
               Luke Metz and
               Soumith Chintala},
  title     = {Unsupervised Representation Learning with Deep Convolutional Generative
               Adversarial Networks},
  booktitle = {ICLR},
  year      = {2016}
}

@inproceedings{10.5555/2969442.2969469,
author = {Frogner, Charlie and Zhang, Chiyuan and Mobahi, Hossein and Araya-Polo, Mauricio and Poggio, Tomaso},
title = {Learning with a {W}asserstein Loss},
year = {2015},
booktitle = {NeurIPS}
}
@incollection{NIPS2018_7827,
title = {Differential Properties of Sinkhorn Approximation for Learning with Wasserstein Distance},
author = {Luise, Giulia and Rudi, Alessandro and Pontil, Massimiliano and Ciliberto, Carlo},
booktitle = {NeurIPS},
year = {2018},
}

@incollection{NIPS2019_9130,
title = {{S}inkhorn Barycenters with Free Support via {Frank-Wolfe} Algorithm},
author = {Luise, Giulia and Salzo, Saverio and Pontil, Massimiliano and Ciliberto, Carlo},
booktitle = {NeurIPS},
year = {2019}
}
@incollection{NIPS2019_8569,
title = {Scalable {G}romov-{W}asserstein Learning for Graph Partitioning and Matching},
author = {Xu, Hongteng and Luo, Dixin and Carin, Lawrence},
booktitle = {NeurIPS},
year = {2019},
}
@article{carlier:hal-00987292,
  TITLE = {{Numerical Methods for Matching for Teams and Wasserstein Barycenters}},
  AUTHOR = {Carlier, Guillaume and Oberman, Adam and Oudet, Edouard},
  JOURNAL = {ESAIM},
  YEAR = {2015}
}

@article{mmotmanifold,
  TITLE = {{Multi-marginal optimal transport on Riemannian manifolds}},
  AUTHOR = {Young-Heon, Kim and Pass, Brendan},
  JOURNAL = {American Journal of Mathematics},
  YEAR = {2015}
}




@inproceedings{dognin2018wasserstein,
title={{W}asserstein Barycenter Model Ensembling},
author={Pierre Dognin and Igor Melnyk and Youssef Mroueh and Jarret Ross and Cicero Dos Santos and Tom Sercu},
booktitle={ICLR},
year={2019}
}
@InProceedings{pmlr-v97-xu19b,
  title = 	 {Gromov-{W}asserstein Learning for Graph Matching and Node Embedding},
  author = 	 {Xu, Hongteng and Luo, Dixin and Zha, Hongyuan and Duke, Lawrence Carin},
  booktitle = 	 {ICML},
  year = 	 {2019},
}

@inproceedings{NIPS2018_7904,
title = {On Gradient Regularizers for {MMD GANs}},
author = {Arbel, Michael and Sutherland, Dougal and  Bińkowski, Mikołaj and Gretton, Arthur},
booktitle = {NeurIPS},
year = {2018}
}


@incollection{NIPS2017_6815,
title = {{MMD GAN}: Towards Deeper Understanding of Moment Matching Network},
author = {Li, Chun-Liang and Chang, Wei-Cheng and Cheng, Yu and Yang, Yiming and Poczos, Barnabas},
booktitle = {NeurIPS},
year = {2017}
}
@inproceedings{DBLP:conf/uai/DziugaiteRG15,
  author    = {Gintare Karolina Dziugaite and
               Daniel M. Roy and
               Zoubin Ghahramani},
  title     = {Training Generative Neural Networks via Maximum Mean Discrepancy Optimization},
  booktitle = {UAI},
  year      = {2015} 
}

@inproceedings{
binkowski2018demystifying,
title={Demystifying {MMD} {GAN}s},
author={Mikolaj Binkowski and Dougal J. Sutherland and Michael Arbel and Arthur Gretton},
booktitle={ICLR},
year={2018}}


@article{benamou:hal-01096124,
  title={Iterative {B}regman Projections for Regularized Transportation Problems},
  author={Benamou, Jean-David and Carlier, Guillaume and Cuturi, Marco and Nenna, Luca and Peyr{\'e}, Gabriel},
  journal={SIAM Journal on Scientific Computing},
  volume={37},
  number={2},
  pages={A1111--A1138},
  year={2015},
  publisher={SIAM}
}


@article{10.1145/2766963,
author = {Solomon, Justin and de Goes, Fernando and Peyr\'{e}, Gabriel and Cuturi, Marco and Butscher, Adrian and Nguyen, Andy and Du, Tao and Guibas, Leonidas},
title = {Convolutional {Wasserstein} Distances: Efficient Optimal Transportation on Geometric Domains},
year = {2015},
journal = {ACM Trans. Graph.}
}

@incollection{NIPS2019_8910,
title = {Differentiable Ranking and Sorting using Optimal Transport},
author = {Cuturi, Marco and Teboul, Olivier and Vert, Jean-Philippe},
booktitle = {NeurIPS},
year = {2019}
}
	
@Article{Kantorovich2006,
author="Kantorovich, L. V.",
title="On the Translocation of Masses",
journal="Journal of Mathematical Sciences",
year="1958"}

@InProceedings{pmlr-v70-arjovsky17a,
  title = 	 {{W}asserstein Generative Adversarial Networks},
  author = 	 {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
  booktitle = 	 {ICML},
  year = 	 {2017}
}
@inproceedings{conf/scalespace/RabinPDB11,
  author = {Rabin, Julien and Peyré, Gabriel and Delon, Julie and Bernot, Marc},
  booktitle = {SSVM},
  title = {Wasserstein Barycenter and Its Application to Texture Mixing},
  year = 2011
}
@article{10.1007/s11222-018-9800-z,
author = {Barrio, E. and Cuesta-Albertos, J. A. and Matr\'{a}n, C. and Mayo-\'{I}scar, A.},
title = {Robust Clustering Tools Based on Optimal Transportation},
year = {2019},
journal = {Statistics and Computing},
}


@inproceedings{gramfort:hal-01135198,
  TITLE = {Fast Optimal Transport Averaging of Neuroimaging Data},
  AUTHOR = {Gramfort, Alexandre and Peyr{\'e}, Gabriel and Cuturi, Marco},
  BOOKTITLE = {IPMI},
  YEAR = {2015}
}
@inproceedings{10.5555/3327757.3327812,
author = {Sanjabi, Maziar and Ba, Jimmy and Razaviyayn, Meisam and Lee, Jason },
title = {On the Convergence and Robustness of Training {GANs} with Regularized Optimal Transport},
year = {2018},
booktitle = {NeurIPS}
}



@article{compopt,
title	= {Computational Optimal Transport},
author	= { Peyr{\'e}, Gabriel and Cuturi, Marco },
year	= {2019},
journal	= {Foundations and Trends in Machine Learning},
}

@article{haasler2020multi,
  title={Multi-marginal Optimal Transport and {S}chr\"{o}dinger Bridges on Trees},
  author={Haasler, Isabel and Ringh, Axel and Chen, Yongxin and Karlsson, Johan},
  journal={arXiv:2004.06909},
  year={2020}
}

@InProceedings{riemannianflow, title = {Riemannian Convex Potential Flows}, author = {Cohen, Samuel and Amos, Brandon and Lipman, Yaron}, booktitle = {Proceedings of the 38th International Conference on Machine Learning}, year = {2021}}

@InProceedings{pmlr-v97-xie19a,
  title = 	 {On Scalable and Efficient Computation of Large Scale Optimal Transport},
  author = 	 {Xie, Yujia and Chen, Minshuo and Jiang, Haoming and Zhao, Tuo and Zha, Hongyuan},
  booktitle = 	 {ICML},  year = 	 {2019}
}

@article{altschuler_mm,
  author    = {Jason M. Altschuler and
               Enric Boix{-}Adser{\`{a}}},
  title     = {Polynomial-time Algorithms for Multimarginal Optimal Transport Problems
               with Structure},
  journal    = {arXiv:2008.03006},
  year       = {2020}
}

@article{altschuler_mm_np,
  author    = {Jason M. Altschuler and
               Enric Boix{-}Adser{\`{a}}},
  title     = {Hardness results for Multimarginal Optimal Transport problems },
  journal    = {arXiv:2012.05398},
  year       = {2020}
}

@article{gym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  journal = {arXiv:1606.01540},
  note = {MIT License}
}


@article{Dadashi2020PrimalWI,
  title={Primal {W}asserstein Imitation Learning},
  author={Robert Dadashi and L'eonard Hussenot and M. Geist and O. Pietquin},
  journal={arXiv:2006.04678},
  year={2020}}

@inproceedings{mmwgan,
title = {Multi-marginal {W}asserstein {GAN}},
author = {Cao, Jiezhang and Mo, Langyuan and Zhang, Yifan and Jia, Kui and Shen, Chunhua and Tan, Mingkui},
booktitle = {NeurIPS},
year = {2019}
}

@inproceedings{bunne2019,
  title={Learning Generative Models across Incomparable Spaces},
  author={Bunne, Charlotte and Alvarez-Melis, David and Krause, Andreas and Jegelka, Stefanie},
  year={2019},
  booktitle={ICML}
}

@inproceedings{10.5555/3045390.3045425,
author = {Reddi, Sashank J. and Hefny, Ahmed and Sra, Suvrit and P\'{o}cz\'{o}s, Barnab\'{a}s and Smola, Alex},
title = {Stochastic Variance Reduction for Nonconvex Optimization},
year = {2016},
publisher = {JMLR.org},
booktitle = {ICML},
numpages = {10},
location = {New York, NY, USA},
series = {ICML’16}
}



@book{Bertsekas/99,
  added-at = {2008-10-07T16:03:39.000+0200},
  author = {Bertsekas, D.P.},
  interhash = {707445f7e287b760fbc42a68a902abca},
  intrahash = {e0a5e65ff1109bdeaa6cfc2a2481e28a},
  keywords = {imported},
  publisher = {Athena Scientific},
  timestamp = {2008-10-07T16:03:40.000+0200},
  title = {Nonlinear Programming},
  year = 1999
}

@article{Pele2009FastAR,
  title={Fast and robust Earth Mover's Distances},
  author={Ofir Pele and Michael Werman},
  journal={ICCV},
  year={2009}
}


@InProceedings{NIPS2017_7159,
title = {Improved Training of {W}asserstein {GANs}},
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
booktitle = {NeurIPS},
year = {2017},
}

@inproceedings{CycleGAN2017,
  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},
  year={2017}
}

@article{vanderouderaaW2019a,
  author = {van der Ouderaa, Tycho F. A. and Worrall, Daniel E.},
  title = {Reversible GANs for Memory-efficient Image-to-Image Translation},
  journal = {CoRR},
  volume = {abs/1902.02729},
  year = {2019},
  archiveprefix = {arXiv},
  eprint = {1902.02729},
  timestamp = {Tue, 21 May 2019 18:03:40 +0200},
}

@inproceedings{Gretton:2005:MSD:2101372.2101382,
 author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Sch\"{o}lkopf, Bernhard},
 title = {Measuring Statistical Dependence with {H}ilbert-{S}chmidt Norms},
 booktitle = {COLT},
 year = {2005}}

@InProceedings{pmlr-v37-rezende15,
  title = 	 {Variational Inference with Normalizing Flows},
  author = 	 {Danilo Rezende and Shakir Mohamed},
  booktitle = 	 {ICML},
  year = 	 {2015},
}

@inproceedings{DBLP:conf/iclr/DinhSB17,
  author    = {Laurent Dinh and
               Jascha Sohl{-}Dickstein and
               Samy Bengio},
  title     = {Density Estimation using Real {NVP}},
  booktitle = {ICLR},
  year      = {2017}
}



@inproceedings{DBLP:journals/corr/DinhKB14,
  author    = {Laurent Dinh and
               David Krueger and
               Yoshua Bengio},
  title     = {{NICE:} Non-linear Independent Components Estimation},
  booktitle = {ICLR},
  year      = {2015},
  crossref  = {DBLP:conf/iclr/2015w},
  timestamp = {Fri, 29 Mar 2019 00:00:00 +0100},
}

@incollection{NIPS2018_7892,
title = {Neural Ordinary Differential Equations},
author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
booktitle = {NeurIPS},
year = {2018},
}

@incollection{monge,
title = {Mémoire sur la Théorie des Déblais et des Remblais},
author = {Monge, Gaspard},
booktitle = {Histoire de l’Académie Royale des Sciences de Paris},
year = {1781}
}

@book{villani,
  title={Optimal Transport: Old and New},
  author={Villani, C{\'e}dric},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@incollection{NIPS2018_8224,
title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
author = {Kingma, Durk P and Dhariwal, Prafulla},
booktitle = {NeurIPS},
year = {2018},
}

@inproceedings{Genevay:2016:SOL:3157382.3157482,
 author = {Genevay, Aude and Cuturi, Marco and Peyr{\'e}, Gabriel and Bach, Francis},
 title = {Stochastic Optimization for Large-scale Optimal Transport},
 booktitle = {NeurIPS},
 year = {2016},
 numpages = {9},
 acmid = {3157482},
 address = {USA},
} 

@inproceedings{Cuturi:2013:SDL:2999792.2999868,
 author = {Cuturi, Marco},
 title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
 booktitle = {NeurIPS},
 year = {2013}
} 

@inproceedings{courty:hal-01018698,
  TITLE = {{Domain adaptation with regularized optimal transport}},
  AUTHOR = {Courty, Nicolas and Flamary, R{\'e}mi and Tuia, Devis},
  BOOKTITLE = {{ECML/PKDD 2014}},
  ADDRESS = {Nancy, France},
  SERIES = {LNCS},
  YEAR = {2014},
  MONTH = Sep,
  PDF = {https://hal.archives-ouvertes.fr/hal-01018698/file/paper.pdf},
  HAL_ID = {hal-01018698},
  HAL_VERSION = {v1},
}
@inproceedings{Wasserstein15,
  author    = {Charlie Frogner and
               Chiyuan Zhang and
               Hossein Mobahi and
               Mauricio Araya{-}Polo and
               Tomaso A. Poggio},
  title     = {Learning with a {W}asserstein Loss},
  booktitle = {NeurIPS},
  year      = {2015}
}
@Article{Mémoli2011,
author="M{\'e}moli, Facundo",
title="Gromov--Wasserstein Distances and the Metric Approach to Object Matching",
journal="Foundations of Computational Mathematics",
year="2011",
month="Aug",
day="01",
volume="11",
number="4",
abstract="This paper discusses certain modifications of the ideas concerning the Gromov--Hausdorff distance which have the goal of modeling and tackling the practical problems of object matching and comparison. Objects are viewed as metric measure spaces, and based on ideas from mass transportation, a Gromov--Wasserstein type of distance between objects is defined. This reformulation yields a distance between objects which is more amenable to practical computations but retains all the desirable theoretical underpinnings. The theoretical properties of this new notion of distance are studied, and it is established that it provides a strict metric on the collection of isomorphism classes of metric measure spaces. Furthermore, the topology generated by this metric is studied, and sufficient conditions for the pre-compactness of families of metric measure spaces are identified. A second goal of this paper is to establish links to several other practical methods proposed in the literature for comparing/matching shapes in precise terms. This is done by proving explicit lower bounds for the proposed distance that involve many of the invariants previously reported by researchers. These lower bounds can be computed in polynomial time. The numerical implementations of the ideas are discussed and computational examples are presented.",
issn="1615-3383",
doi="10.1007/s10208-011-9093-5",
}

@online{xiao2017/online,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}
@article{lecun-mnisthandwrittendigit-2010,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},

  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  username = {mhwombat},
  year = 2010
}

@inproceedings{alvarez-melis-jaakkola-2018-gromov,
    title = "{G}romov-{W}asserstein Alignment of Word Embedding Spaces",
    author = "Alvarez-Melis, David  and
      Jaakkola, Tommi",
    booktitle = "EMNLP",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    abstract = "Cross-lingual or cross-domain correspondences play key roles in tasks ranging from machine translation to transfer learning. Recently, purely unsupervised methods operating on monolingual embeddings have become effective alignment tools. Current state-of-the-art methods, however, involve multiple steps, including heuristic post-hoc refinement strategies. In this paper, we cast the correspondence problem directly as an optimal transport (OT) problem, building on the idea that word embeddings arise from metric recovery algorithms. Indeed, we exploit the Gromov-Wasserstein distance that measures how similarities between pairs of words relate across languages. We show that our OT objective can be estimated efficiently, requires little or no tuning, and results in performance comparable with the state-of-the-art in various unsupervised word translation tasks.",
}

@Misc{
  bonnottee,
  author = { Nicolas Bonnotte },
  title = { Unidimensional and Evolution Methods for Optimal Transportation },
  journal = {  },
  year = { 2013 },
  pages = {  },
}
@InProceedings{slicedbernot,
author="Rabin, Julien
and Peyr{\'e}, Gabriel
and Delon, Julie
and Bernot, Marc",
title="Wasserstein Barycenter and Its Application to Texture Mixing",
booktitle="Scale Space and Variational Methods in Computer Vision",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="435--446",
isbn="978-3-642-24785-9"
}

@article{knottmmot,
title = {On a generalization of cyclic monotonicity and distances among random vectors},
journal = {Linear Algebra and its Applications},
volume = {199},
pages = {363-371},
year = {1994},
note = {Special Issue Honoring Ingram Olkin},
issn = {0024-3795},
doi = {https://doi.org/10.1016/0024-3795(94)90359-X},
url = {https://www.sciencedirect.com/science/article/pii/002437959490359X},
author = {M. Knott and C.S. Smith},
abstract = {We generalize an investigation of distances among random vectors begun by Olkin and Pukelsheim and continued by Olkin and Rachev.}
}

@article{Solomon16,
  Author = {Justin Solomon and Gabriel Peyre and Vladimir G. Kim and Suvrit Sra},
  Journal = {SIGGRAPH},
  Title = {Entropic Metric Alignment for Correspondence Problems},
  Year = {2016}
}
@InProceedings{gwaveraging,
  title = 	 {{Gromov-Wasserstein} Averaging of Kernel and Distance Matrices},
  author = 	 {Gabriel Peyré and Marco Cuturi and Justin Solomon},
  booktitle = 	 {ICML},
  year = 	 {2016}
}
@InProceedings{orthsliced,
  title = 	 {Orthogonal Estimation of {W}asserstein Distances},
  author = 	 {Mark Rowland and Jiri Hron and Yunhao Tang and Krzysztof Choromanski and Tamas Sarlos and Adrian Weller},
  year = 	 {2019},
  booktitle = {AISTATS}
}

@incollection{mmmonge,
  title={A numerical method to solve multi-marginal optimal transport problems with {C}oulomb cost},
  author={Benamou, Jean-David and Carlier, Guillaume and Nenna, Luca},
  booktitle={Splitting Methods in Communication, Imaging, Science, and Engineering},
  year={2016},
  publisher={Springer}
}

@inproceedings{
slicedkernel,
title={Sliced Kernelized Stein Discrepancy},
author={Wenbo Gong and Yingzhen Li and Jos{\'e} Miguel Hern{\'a}ndez-Lobato},
booktitle={International Conference on Learning Representations},
year={2021},
}

@inproceedings{mwgan,
 author = {Cao, Jiezhang and Mo, Langyuan and Zhang, Yifan and Jia, Kui and Shen, Chunhua and Tan, Mingkui},
 booktitle = {NeurIPS},
 title = {Multi-marginal Wasserstein GAN},
 year = {2019}
}


@inproceedings{
sliced_fgw,
title={Improving Relational Regularized Autoencoders with Spherical Sliced Fused Gromov Wasserstein},
author={Khai Nguyen and Son Nguyen and Nhat Ho and Tung Pham and Hung Bui},
booktitle={ICLR},
year={2021},
}

@inproceedings{
distributionalsliced,
title={Distributional Sliced-Wasserstein and Applications to Generative Modeling},
author={Khai Nguyen and Nhat Ho and Tung Pham and Hung Bui},
booktitle={ICLR},
year={2021},
}

@article{
g.2018the,
title={The {C}ramer Distance as a Solution to Biased {W}asserstein Gradients},
author={Marc G. Bellemare and Ivo Danihelka and Will Dabney and Shakir Mohamed and Balaji Lakshminarayanan and Stephan Hoyer and Remi Munos},
journal = {arXiv:1705.10743},
year={2018}
}
@article{gromovmonge,
title={Gromov-Monge quasi-metrics and distance distributions},
author={Facundo Mémoli and Tom Needham},
journal = {arXiv:1810.09646},
year={2018}
}



@article{
generalizedmet,
title={Multi-Marginal Optimal Transport Defines a Generalized Metric
},
author={José Bento and Liang Mi},
journal = {arXiv:2001.11114},
year={2020}
}
@book{hardy1952inequalities,
  title={Inequalities},
  author={Hardy, G.H. and Karreman Mathematics Research Collection and Littlewood, J.E. and P{\'o}lya, G. and P{\'o}lya, G. and Littlewood, D.E.},
  year={1952},
  publisher={Cambridge University Press}
}
@article{
sl_prop,
title={Statistical And Topological Properties of Sliced Probability Divergences},
author={Kimia Nadjahi and
Alain Durmus and  Lénaïc Chizat and  Soheil Kolouri and Shahin Shahrampour and Umut ¸Simsekli},
journal = {arXiv:2003.05783},
year={2020}
}


@article{
rigolletbures,
title={Gradient Descent Algorithms for Bures-Wasserstein Barycenters
},
author={Chewi, Sinho and Maunu, Tyler and  Rigollet, Philippe and Stromme , Austin J. },
journal = {arXiv:2001.01700},
year={2020}
}


@incollection{NIPS2014_5423,
title = {Generative Adversarial Nets},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {NeurIPS},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
year = {2014}
}
@misc{radford2015unsupervised,
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has
seen huge adoption in computer vision applications. Comparatively, unsupervised
learning with CNNs has received less attention. In this work we hope to help
bridge the gap between the success of CNNs for supervised learning and
unsupervised learning. We introduce a class of CNNs called deep convolutional
generative adversarial networks (DCGANs), that have certain architectural
constraints, and demonstrate that they are a strong candidate for unsupervised
learning. Training on various image datasets, we show convincing evidence that
our deep convolutional adversarial pair learns a hierarchy of representations
from object parts to scenes in both the generator and discriminator.
Additionally, we use the learned features for novel tasks - demonstrating their
applicability as general image representations.},
  added-at = {2018-09-19T23:39:30.000+0200},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  description = {Unsupervised Representation Learning with Deep Convolutional Generative
  Adversarial Networks},
  title = {Unsupervised Representation Learning with Deep Convolutional Generative
  Adversarial Networks},
  year = 2015
}
@article{DBLP:journals/corr/LedigTHCATTWS16,
  author    = {Christian Ledig and
               Lucas Theis and
               Ferenc Huszar and
               Jose Caballero and
               Andrew P. Aitken and
               Alykhan Tejani and
               Johannes Totz and
               Zehan Wang and
               Wenzhe Shi},
  title     = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial
               Network},
  journal   = {CoRR},
  volume    = {abs/1609.04802},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1609.04802},
  timestamp = {Mon, 13 Aug 2018 16:48:38 +0200},
}
@article{pix2pix2017,
  title={Image-to-Image Translation with Conditional Adversarial Networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  journal={CVPR},
  year={2017}
}
@incollection{NIPS2016_6544,
title = {Coupled Generative Adversarial Networks},
author = {Liu, Ming-Yu and Tuzel, Oncel},
booktitle = {NeurIPS},
editor = { Lee and  Sugiyama and Luxburg and  Guyon and  Garnett},
year = {2016}
}

@incollection{NIPS2016_6399,
title = {InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
booktitle = {NeurIPS},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
year = {2016},
}

@incollection{NIPS2017_7137,
title = {Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference},
author = {Kumar, Abhishek and Sattigeri, Prasanna and Fletcher, Tom},
booktitle = {NeurIPS},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
year = {2017},

}



@inproceedings{vanderOuderaa2019revgan,
  title={Reversible GANs for Memory-efficient Image-to-Image Translation},
  author={van der Ouderaa, Worrall, Daniel E},
  booktitle={CVPR 2019},
  year={2019}
}
@inproceedings{CycleGAN2017,
  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},
  year={2017}
}

@article{PfiBuhSchPet18,
  title = {Kernel-based tests for joint independence},
  author = {Pfister, N. and B{\"u}hlmann, P. and Sch{\"o}lkopf, B. and Peters, J.},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {80},
  number = {1},
  year = {2018}
}

@incollection{NIPS2013_4893,
title = {A Kernel Test for Three-Variable Interactions},
author = {Sejdinovic, Dino and Gretton, Arthur and Bergsma, Wicher},
booktitle = {NeurIPS},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
year = {2013}}

@article{Bengio:2013:RLR:2498740.2498889,
 author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
 title = {Representation Learning: A Review and New Perspectives},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 volume = {35},
 number = {8},
 month = aug,
 year = {2013},
 issn = {0162-8828},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Learning systems,Machine learning,Abstracts,Feature extraction,Manifolds,Neural networks,Speech recognition,neural nets,Deep learning,representation learning,feature learning,unsupervised learning,Boltzmann machine,autoencoder},
} 
@inproceedings{Higgins2017betaVAELB,
  title={beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  author={Irina Higgins and Lo{\"i}c Matthey and Arka Pal and Christopher Burgess and Xavier Glorot and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner},
  booktitle={ICLR},
  year={2017}
}





@InProceedings{pmlr-v80-kim18b,
  title = 	 {Disentangling by Factorising},
  author = 	 {Kim, Hyunjik and Mnih, Andriy},
  booktitle = 	 {ICML},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kim18b/kim18b.pdf},
  abstract = 	 {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon beta-VAE by providing a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.}
}
@incollection{Chen2016InfoGAN,
title = {{InfoGAN}: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
author = {Chen, Xi and Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
booktitle = {NeurIPS},
editor = {D. D. Lee and U. V. Luxburg and I. Guyon and R. Garnett},
year = {2016},

}
@inproceedings{inproceedings,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
year = {2016},
month = {06},
title = {Deep Residual Learning for Image Recognition},
doi = {10.1109/CVPR.2016.90}
}
@inproceedings{DBLP:conf/aaai/GroverDE18,
  author    = {Aditya Grover and
               Manik Dhar and
               Stefano Ermon},
  title     = {Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in
               Generative Models},
  booktitle = {AAAI},
  year      = {2018},
  crossref  = {DBLP:conf/aaai/2018},
  timestamp = {Tue, 23 Oct 2018 06:42:15 +0200},
}
@misc{mirza2014conditional,
  
  added-at = {2017-10-04T17:14:40.000+0200},
  author = {Mirza, Mehdi and Osindero, Simon},
  description = {Conditional Generative Adversarial Nets},
  interhash = {efbbaeaebb1ea8d88264d258624d364c},
  intrahash = {a4426d639ebb30270839ad347bcfb999},
  keywords = {2014 GAN deep-learning machine-learning neural-networks},
  note = {cite arxiv:1411.1784},
  timestamp = {2017-10-04T17:14:40.000+0200},
  title = {Conditional Generative Adversarial Nets},
  year = 2014
}



@article{DBLP:journals/corr/abs-1905-12892,
  author    = {Aditya Grover and
               Christopher Chute and
               Rui Shu and
               Zhangjie Cao and
               Stefano Ermon},
  title     = {AlignFlow: Cycle Consistent Learning from Multiple Domains via Normalizing
               Flows},
  journal   = {CoRR},
  volume    = {abs/1905.12892},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1905.12892},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mansimov16_text2image,
  author    = {Elman Mansimov and Emilio Parisotto and Jimmy Ba and Ruslan Salakhutdinov},
  title     = {Generating Images from Captions with Attention},
  booktitle = {ICLR},
  year      = {2016}
}






@InProceedings{pmlr-v97-locatello19a,
  title = 	 {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
  author = 	 {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  booktitle = 	 {ICML},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/locatello19a/locatello19a.pdf},
  abstract = 	 {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than $12000$ models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties “encouraged” by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.}
}

@inproceedings{liu2015faceattributes,
 title = {Deep Learning Face Attributes in the Wild},
 author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
 booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
 month = {December},
 year = {2015} 
}
@inproceedings{mansimov16_text2image,
  author    = {Elman Mansimov and Emilio Parisotto and Jimmy Ba and Ruslan Salakhutdinov},
  title     = {Generating Images from Captions with Attention},
  booktitle = {ICLR},
  year      = {2016}
}

@incollection{NIPS2013_5021,
title = {Distributed Representations of Words and Phrases and their Compositionality},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
booktitle = {NeurIPS},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
year = {2013},
}


@incollection{semi_imp,
title = {Semi-Implicit Generative Model},
author = {Yin,Mingzhang and Zhou,Mingyuan},
booktitle = {Workshop on Bayesian Deep Learning, NeurIPS 2018},
year = {2018}

}
@article{mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  journal = {Nature},
  number = 7540,
  pages = {529--533},
  title = {Human-level control through deep reinforcement learning},
  volume = 518,
  year = 2015
}





@book{appliedmath,
  author = {Santambrogio, Filippo},
  title = {Optimal Transport for Applied Mathematicians},
  year = 2015,
  publisher = {Springer}
}


@article{journals/focm/Memoli11,
  author = {Mémoli, Facundo},
  journal = {Foundations of Computational Mathematics},
  title = {{Gromov-Wasserstein }Distances and the Metric Approach to Object Matching.},
  year = 2011
}

@incollection{sliced_gw,
title = {{Sliced Gromov-Wasserstein}},
author = {Vayer, Titouan  and Flamary, R\'{e}mi and Courty, Nicolas and Tavenard, Romain and Chapel, Laetitia},
booktitle = {NeurIPS},
year = {2019}
}



@Book{ trove.nla.gov.au/work/30144119,
title = { Reproducing kernel Hilbert spaces in probability and statistics },
author = { Berlinet, A and Thomas-Agnan, Christine },
publisher = { Boston, Mass. : Kluwer Academic },
year = { 2004 },
type = { Book; Book/Illustrated },
isbn = { 1402076797 },
subjects = { Hilbert space; Kernel functions; Probabilities; Mathematical statistics },
language = { English },
note = { Includes bibliographical references (p. 327-343) and index },
contents = { 1. Theory -- 2. RKHS and Stochastic Processes -- 3. Nonparametric Curve Estimation -- 4. Measures and Random Measures -- 5. Miscellaneous Applications -- 6. Computational Aspects -- 7. A Collection of Examples }
}


@inproceedings{Flaxman:2016:BLK:3020948.3020968,
 author = {Flaxman, Seth and Sejdinovic, Dino and Cunningham, John P. and Filippi, Sarah},
 title = {Bayesian Learning of Kernel Embeddings},
 booktitle = {UAI},
 year = {2016},
 location = {Jersey City, New Jersey, USA},
 numpages = {10},
 acmid = {3020968},
 publisher = {AUAI Press},
 address = {Arlington, Virginia, United States},
} 

@inproceedings{imagenet_cvpr09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}
        
        @incollection{NIPS2012_4824,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {NeurIPS},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
year = {2012},
}
@inproceedings{Girshick:2014:RFH:2679600.2679851,
 author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
 title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
 series = {CVPR '14},
 year = {2014},
 isbn = {978-1-4799-5118-5},
 numpages = {8},
 doi = {10.1109/CVPR.2014.81},
 acmid = {2679851},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 
@article{LeCun:1989:BAH:1351079.1351090,
 author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
 title = {Backpropagation Applied to Handwritten Zip Code Recognition},
 journal = {Neural Comput.},
 issue_date = {Winter 1989},
 volume = {1},
 number = {4},
 month = dec,
 year = {1989},
 issn = {0899-7667},
 numpages = {11},
 doi = {10.1162/neco.1989.1.4.541},
 acmid = {1351090},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 



@incollection{NIPS2016_6544,
title = {Coupled Generative Adversarial Networks},
author = {Liu, Ming-Yu and Tuzel, Oncel},
booktitle = {NeurIPS},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
year = {2016}}


@inproceedings{Flaxman:2016:BLK:3020948.3020968,
 author = {Flaxman, Seth and Sejdinovic, Dino and Cunningham, John P. and Filippi, Sarah},
 title = {Bayesian Learning of Kernel Embeddings},
 booktitle = {Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence},
 series = {UAI'16},
 year = {2016},
 isbn = {978-0-9966431-1-5},
 location = {Jersey City, New Jersey, USA},
 numpages = {10},
 acmid = {3020968},
 publisher = {AUAI Press},
 address = {Arlington, Virginia, United States},
} 
@article{doi:10.1002/nav.3800030109,
author = {Frank, Marguerite and Wolfe, Philip},
title = {An algorithm for quadratic programming},
journal = {Naval Research Logistics Quarterly},
volume = {3},
number = {1‐2},
year = {1956}
}

@incollection{NIPS2013_4893,
title = {A Kernel Test for Three-Variable Interactions},
author = {Sejdinovic, Dino and Gretton, Arthur and Bergsma, Wicher},
booktitle = {NeurIPS},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
year = {2013},

}



@article{Sriperumbudur:2011:UCK:1953048.2021077,
 author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Lanckriet, Gert R. G.},
 title = {Universality, Characteristic Kernels and RKHS Embedding of Measures},
 journal = {Journal of Machine Learning Research},
 issue_date = {2/1/2011},
 volume = {12},
 month = jul,
 year = {2011},
 numpages = {22}
} 


@inproceedings{mansimov16_text2image,
  author    = {Elman Mansimov and Emilio Parisotto and Jimmy Ba and Ruslan Salakhutdinov},
  title     = {Generating Images from Captions with Attention},
  booktitle = {ICLR},
  year      = {2016}
}



@inproceedings{liu2015faceattributes,
 title = {Deep Learning Face Attributes in the Wild},
 author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
 booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
 month = {December},
 year = {2015} 
}

@inproceedings{DBLP:journals/corr/KingmaW13,
  author    = {Diederik P. Kingma and
               Max Welling},
  title     = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16},
  year      = {2014},
  crossref  = {DBLP:conf/iclr/2014},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200}
}

@article{bayes,
author = {Srivastava, Sanvesh and Li, Cheng and Dunson, David B.},
title = {Scalable Bayes via Barycenter in Wasserstein Space},
year = {2018},
volume = {19},
number = {1},
journal = {Journal of Machine Learning Research},
month = jan,
pages = {312–346}
}


@InProceedings{pmlr-v32-rezende14,
  title = 	 {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  author = 	 {Danilo Jimenez Rezende and Shakir Mohamed and Daan Wierstra},
  booktitle = 	 {Proceedings of the International Conference on Machine Learning},
  year = 	 {2014},
  volume = 	 {32},
  number =       {2},
  abstract = 	 {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.}
}

@article{Zhang:2018:LKM:3177966.3177981,
 author = {Zhang, Qinyi and Filippi, Sarah and Gretton, Arthur and Sejdinovic, Dino},
 title = {Large-scale Kernel Methods for Independence Testing},
 journal = {Statistics and Computing},
 issue_date = {2018},
 volume = {28},
 number = {1},
 year = {2018},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {Hilbert---Schmidt independence criteria, Independence testing, Large-scale kernel method, Nystr\"{o}m method, Random Fourier features},
} 


@article{gangbo,
author = {Gangbo, Wilfrid and Święch, Andrzej},
title = {Optimal maps for the multidimensional Monge-Kantorovich problem},
journal = {Communications on Pure and Applied Mathematics},
year = {1998},
volume = {51},
number = {1},
pages = {23-45}
}


@InProceedings{pmlr-v97-titouan19a,
  title = 	 {Optimal Transport for Structured Data with Application on Graphs},
  author = 	 {Vayer, Titouan and Courty, Nicolas and Tavenard, Romain and Laetitia, Chapel and Flamary, R{\'e}mi},
  booktitle = 	 {ICML},
  year = 	 {2019}
  }

@article{MuaFukSriSch17,
  title = {Kernel Mean Embedding of Distributions: A Review and Beyond},
  author = {Muandet, K. and Fukumizu, K. and Sriperumbudur, B. and Sch{\"o}lkopf, B.},
  journal = {Foundations and Trends in Machine Learning},
  number = {1-2},
pages = {1-141},
volume = {10},

  year = {2017}
}
@inproceedings{Chwialkowski:2014:KIT:3044805.3045051,
 author = {Chwialkowski, Kacper and Gretton, Arthur},
 title = {A Kernel Independence Test for Random Processes},
 booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
 series = {ICML'14},
 year = {2014},
 location = {Beijing, China},
 publisher = {JMLR.org},
} 
@inproceedings{thanh-tung2018improving,
title={Improving Generalization and Stability of Generative Adversarial Networks},
author={Hoang Thanh-Tung and Truyen Tran and Svetha Venkatesh},
booktitle={International Conference on Learning Representations},
year={2019}}


@incollection{NIPS2007_3182,
title = {Random Features for Large-Scale Kernel Machines},
author = {Ali Rahimi and Recht, Benjamin},
booktitle = {NeurIPS},
editor = {J. C. Platt and D. Koller and Y. Singer and S. T. Roweis},
year = {2008},
}


@incollection{NIPS2000_1866,
title = {Using the Nystr\"{o}m Method to Speed Up Kernel Machines},
author = {Christopher K. I. Williams and Matthias Seeger},
booktitle = {NeurIPS},
editor = {T. K. Leen and T. G. Dietterich and V. Tresp},
year = {2001},
publisher = {MIT Press},
}


@InProceedings{Srivastava2015,
  author    = {Sanvesh Srivastava and Volkan Cevher and Quoc Dinh and David Dunson},
  title     = {{WASP: S}calable {B}ayes via Barycenters of Subset Posteriors},
  booktitle = {AISTATS},
  year      = {2015},
  abstract  = {The promise of Bayesian methods for big data sets has not fully been realized due to the lack of scalable computational algorithms. For massive data, it is necessary to store and process subsets on different machines in a distributed manner. We propose a simple, general, and highly efficient approach, which first runs a posterior sampling algorithm in parallel on different machines for subsets of a large data set. To combine these subset posteriors, we calculate the Wasserstein barycenter via a highly efficient linear program. The resulting estimate for the Wasserstein posterior (WASP) has an atomic form, facilitating straightforward estimation of posterior summaries of functionals of interest. The WASP approach allows posterior sampling algorithms for smaller data sets to be trivially scaled to huge data. We provide theoretical justification in terms of posterior consistency and algorithm efficiency.  Examples are provided in complex settings including Gaussian process regression and nonparametric Bayes mixture models.}
}
@inproceedings{NIPS2019_9606,
title = {Exact Gaussian Processes on a Million Data Points},
author = {Wang, Ke and Pleiss, Geoff and Gardner, Jacob and Tyree, Stephen and Weinberger, Kilian Q and Wilson, Andrew Gordon},
booktitle = {NeurIPS},
year = {2019},
}

@Article{Bertone2019,
  author       = {Gianfranco Bertone and Marc P. Deisenroth and Jong S. Kim and Sebastian Liem and Roberto {Ruiz de Austri} and Max Welling},
  journaltitle = {Physics of the Dark Universe},
  title        = {Accelerating the {BSM} Interpretation of {LHC} Data with Machine Learning},
  journal      = {Physics of the Dark Universe},
  year         = {2019},
}

@incollection{NIPS2013_4927,
title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
author = {Cuturi, Marco},
booktitle = {NeurIPS},
year = {2013}
}

@incollection{NIPS2017_7149,
title = {Learning from uncertain curves: The 2-{W}asserstein metric for {G}aussian processes},
author = {Mallasto, Anton and Feragen, Aasa},
booktitle = {NeurIPS},
year = {2017}
}

@article{pub.1023863441,
 author = {Álvarez-Esteban, Pedro C. and del Barrio, E. and Cuesta-Albertos, J. A. and Matrán, C.},
 journal = {Journal of Mathematical Analysis and Applications},
 keywords = {},
 title = {A fixed-point approach to barycenters in {W}asserstein space},
 year = {2016}
}




@book{Rasmussen:2005:GPM:1162254,
 author = {Rasmussen, Carl E. and Williams, Christopher K. I.},
 title = {Gaussian Processes for Machine Learning},
 year = {2006},
 publisher = {The MIT Press},
} 

@article{rulliere:hal-01345959,
  TITLE = {Nested {K}riging predictions for datasets with large number of observations},
  AUTHOR = {Rulli{\`e}re, Didier and Durrande, Nicolas and Bachoc, Fran{\c c}ois and Chevalier, Cl{\'e}ment},
  JOURNAL = {{Statistics and Computing}},
  PUBLISHER = {{Springer Verlag}},
  YEAR = {2018}
}

@InProceedings{pmlr-v37-deisenroth15,
  title = 	 {Distributed {G}aussian Processes},
  author = 	 {Marc P. Deisenroth and Jun W. Ng},
  booktitle = 	 {ICML},
   year = 	 {2015},
   abstract = 	 {To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model for large-scale distributed GP regression. Unlike state-of-the-art sparse GP approximations, the rBCM is conceptually simple and does not rely on inducing or variational parameters. The key idea is to recursively distribute computations to independent computational units and, subsequently, recombine them to form an overall result. Efficient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint. The rBCM is independent of the computational graph and can be used on heterogeneous computing infrastructures, ranging from laptops to clusters. With sufficient computing resources our distributed GP model can handle arbitrarily large data sets.}
}

@inproceedings{trapp2019structured,
  added-at = {2019-10-17T02:58:20.000+0200},
  author = {Trapp, Martin and Peharz, Robert and Pernkopf, Franz and Rasmussen, Carl E.},
  title = {Deep Structured Mixtures of {G}aussian Processes},
  year = {2019},
  booktitle={AISTATS}
}



@inproceedings{Hensman:2013:GPB:3023638.3023667,
 author = {Hensman, James and Fusi, Nicol\`{o} and Lawrence, Neil D.},
 title = {Gaussian Processes for Big Data},
 booktitle = {UAI},
 year = {2013}
} 

@article{MAL-073,
year = {2019},
journal = {Foundations and Trends® in Machine Learning},
title = {Computational Optimal Transport},
author = {Gabriel Peyré and Marco Cuturi}
}

@INPROCEEDINGS{Titsias09variationallearning,
    author = {Michalis K. Titsias},
    title = {Variational learning of inducing variables in sparse {G}aussian processes},
    booktitle = {AISTATS},
    year = {2009},
}



@InProceedings{pmlr-v37-wilson15,
  title = 	 {Kernel Interpolation for Scalable Structured {G}aussian Processes ({KISS-GP})},
  author = 	 {Andrew G. Wilson and Hannes Nickisch},
  booktitle = 	 {ICML},
  year = 	 {2015},
   abstract = 	 {We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling.}
}

@article{wilson2015msgp,
  title={Thoughts on Massively Scalable {G}aussian Processes},
  author={Wilson, Andrew G. and Dann, Christoph and Nickisch, Hannes},
  journal={arXiv:1511.01870},
   year={2015}
}

@article{liu2018generalized,
  title={Generalized robust {B}ayesian committee machine for large-scale {G}aussian process regression},
  author={Liu, Haitao and Cai, Jianfei and Wang, Yi and Ong, Yew-Soon},
  journal={arXiv:1806.00720},
  year={2018}
}

@inproceedings{NIPS2005_2857,
title = {Sparse {G}aussian Processes using Pseudo-inputs},
author = {Edward Snelson and Ghahramani, Zoubin},
booktitle = {NeurIPS},
year = {2006}
}


@Article{Guest2018,
  author        = {Guest, Dan and Cranmer, Kyle and Whiteson, Daniel},
  title         = {{Deep Learning and its Application to LHC Physics}},
  year          = {2018},
  archiveprefix = {arXiv},
  eprint        = {1806.11484},
  journaltitle  = {arXiv:1806.11484},
  primaryclass  = {hep-ex},
}
@Article{Roberts2013,
  author    = {Roberts, Stephen and Osborne, Michael A. and Ebden, Mark and Reece, Steven and Gibson, Neale and Aigrain, Suzanne},
  title     = {{Gaussian {Processes for Time Series Modelling}}},
  number    = {1984},
  volume    = {371},
  abstract  = {In this paper, we offer a gentle introduction to Gaussian processes for time-series data analysis. The conceptual framework of Bayesian modelling for time-series data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches.},
  journal   = {Philosophical Transactions of the Royal Society (Part A)},
  timestamp = {2013.11.01},
  year      = {2013},
}

@Article{Castruccio2014,
  author       = {Castruccio, Stefano and McInerney, David J. and Stein, Michael L. and Liu Crouch, Feifei and Jacob, Robert L. and Moyer, Elisabeth J.},
  year         = {2014},
  journaltitle = {Journal of Climate}
}


@article{soton259182,
           title = {Sparse Online {G}aussian Processes},
          author = {Lehel Csato and Manfred Opper},
            year = {2002},
         journal = {Neural Computation},
        abstract = {We develop an approach for sparse representations of Gaussian Process (GP) models (which are Bayesian types of kernel machines) in order to overcome their limitations caused by large data sets. The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the GP model. By using an appealing parametrisation and projection techniques that use the RKHS norm, recursions for the effective parameters and a sparse Gaussian approximation of the posterior process are obtained. This allows both for a propagation of predictions as well as of Bayesian error measures. The significance and robustness of our approach is demonstrated on a variety of experiments.}
}


@article{10.5555/1046920.1194909,
author = {Qui\~{n}onero-Candela, Joaquin and Rasmussen, Carl Edward},
title = {A Unifying View of Sparse Approximate {G}aussian Process Regression},
year = {2005},
journal = {JMLR},
numpages = {21}
}



@inproceedings{NIPS2014_5593,
title = {Distributed Variational Inference in Sparse {G}aussian Process Regression and Latent Variable Models},
author = {Gal, Yarin and van der Wilk, Mark and Rasmussen, Carl E.},
booktitle = {NeurIPS},
year = {2014}
}


@article{Tresp2000,
author = {Tresp, Volker},
title = {A {B}ayesian Committee Machine},
year = {2000},
issue_date = {November 2000},
publisher = {MIT Press},
journal = {Neural Computation},
}



@article{Cao2015TransductiveLO,
  title={Transductive Log Opinion Pool of {G}aussian Process Experts},
  author={Yanshuai Cao and David J. Fleet},
  journal={arXiv:1511.07551},
  year={2015}
}
@article{JMLR:v20:18-374,
  author  = {Michael Minyi Zhang and Sinead A. Williamson},
  title   = {Embarrassingly Parallel Inference for {G}aussian Processes},
  journal = {Journal of Machine Learning Research},
  year    = {2019}
}
@article{Cao2014GeneralizedPO,
  title={Generalized Product of Experts for Automatic and Principled Fusion of {G}aussian Process Predictions},
  author={Yanshuai Cao and David J. Fleet},
  journal={arXiv:1410.7827},
  year={2014},
}

@inproceedings{DBLP:conf/nips/RasmussenG01,
  author={Carl E. Rasmussen and Zoubin Ghahramani},
  title={Infinite Mixtures of {G}aussian Process Experts},
  year={2001},
  booktitle={NeurIPS}
  }
  
  @InProceedings{Wang2019,
  author    = {Wang, Ke Alexander and Pleiss, Geoff and Gardner, Jacob R. and Weinberger, Kilian Q. and Wilson, Andrew G.},
  title     = {Exact {G}aussian Processes on a Million Data Points},
  booktitle = {NeurIPS},
  year      = {2019}
}

@inproceedings{10.5555/3008751.3008843,
author = {Tresp, Volker},
title = {Mixtures of {G}aussian Processes},
year = {2000},
booktitle = {NeurIPS},
}



@InProceedings{pmlr-v38-hensman15,
  title = 	 {Scalable Variational {G}aussian Process Classification},
  author = 	 {James Hensman and Alexander G. de G. Matthews and Zoubin Ghahramani},
  booktitle = 	 {AISTATS},
  year = 	 {2015},
  abstract = 	 {Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, out-performing the state of the art on benchmark datasets. Importantly, the variational formulation an be exploited to allow classification in problems with millions of data points, as we demonstrate in experiments.}
}
@inproceedings{hinton1999products,
  title={Products of Experts},
  author={Hinton, Geoffrey E.},
  year={1999},
  booktitle={ICANN}
}


@article{doi:10.1002/mana.19901470121,
author = {Gelbrich, Matthias},
title = {On a Formula for the {L2 W}asserstein Metric between Measures on {E}uclidean and {H}ilbert Spaces},
journal = {Mathematische Nachrichten},
year = {1990}
}
@inproceedings{NIPS2016_6477,
title = {Understanding Probabilistic Sparse {G}aussian Process Approximations},
author = {Bauer, Matthias and van der Wilk, Mark and Rasmussen, Carl Edward},
booktitle = {NIPS},
year = {2016}
}
@incollection{gensliced,
title = {Generalized Sliced {W}asserstein Distances},
author = {Kolouri, Soheil and Nadjahi, Kimia and Simsekli, Umut and Badeau, Roland and Rohde, Gustavo},
booktitle = {NeurIPS},
year = {2019}
}

@article{kolouri2018slicedwasserstein,
  author = {Kolouri, Soheil and Martin, Charles E. and Rohde, Gustavo K.},
  title = {Sliced-{W}asserstein Autoencoder: An Embarrassingly Simple Generative
  Model},
  journal = {arXiv:1804.01947},
  year = 2018
}
@inproceedings{pilco,
  author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
  booktitle = {ICML},
  title = {PILCO: A Model-Based and Data-Efficient Approach to Policy Search.},
  year = 2011
}


@Book{Bert05,
  Title                    = {Dynamic Programming and Optimal Control},
  Author                   = {Dimitri P. Bertsekas},
  Publisher                = {Athena Scientific},
  Year                     = {2005},

  Address                  = {Belmont, MA, USA},
  Edition                  = {3rd},
  Volume                   = {I}
}
@inproceedings{maxsliced,
  added-at = {2019-04-25T00:00:00.000+0200},
  author = {Deshpande, Ishan and Hu, Yuan-Ting and Sun, Ruoyu and Pyrros, Ayis and Siddiqui, Nasir and Koyejo, Sanmi and Zhao, Zhizhen and Forsyth, David A. and Schwing, Alexander G.},
  booktitle = {CVPR},
   title = {Max-Sliced Wasserstein Distance and Its Use for GANs},
  year = 2019
}

@inproceedings{deshpande2019max,
  title={Max-sliced {W}asserstein distance and its use for {GAN}s},
  author={Deshpande, Ishan and Hu, Yuan-Ting and Sun, Ruoyu and Pyrros, Ayis and Siddiqui, Nasir and Koyejo, Sanmi and Zhao, Zhizhen and Forsyth, David and Schwing, Alexander G},
  booktitle={CVPR},
  year={2019}
}


@ARTICLE {bonneel2013sliced,
    title={Sliced {W}asserstein Barycenter of Multiple Densities},
    author={Bonneel, Nicolas and Pfister, Hanspeter},
    journal={Harvard Technical Report},
    year={2013}
}

@inproceedings{
dognin2018wasserstein,
title={Wasserstein Barycenter Model Ensembling},
author={Pierre Dognin and Igor Melnyk and Youssef Mroueh and Jarret Ross and Cicero Dos Santos and Tom Sercu},
booktitle={ICLR},
year={2019},
}

@article{10.1016/j.jmva.2007.01.004,
author = {Choi, Taeryon and Schervish, Mark J.},
title = {On Posterior Consistency in Nonparametric Regression Problems},
year = {2007},
issue_date = {November, 2007},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {98},
number = {10},
issn = {0047-259X},
url = {https://doi.org/10.1016/j.jmva.2007.01.004},
doi = {10.1016/j.jmva.2007.01.004},
journal = {J. Multivar. Anal.},
month = nov,
pages = {1969–1987},
numpages = {19},
keywords = {Secondary, Differentiable functions, In probability metric, Hellinger metric, Almost sure consistency, Empirical probability measure, Sieve, Primary}
}



  

@article{saoke78,
  author = {Sakoe, Hiroaki and Chiba, Seibi},
  journal = {ICASSP},
  title = {Dynamic Programming Algorithm Optimization for Spoken Word Recognition},
  year = 1978
}

@article{doi:10.1002/nav.3800030109,
author = {Frank, Marguerite and Wolfe, Philip},
title = {An Algorithm for Quadratic Programming},
journal = {Naval Research Logistics Quarterly},
volume = {3},
number = {1--2},
pages = {95--110},
year = {1956}
}
@article{DBLP:journals/corr/Lacoste-Julien16,
  author    = {Simon Lacoste{-}Julien},
  title     = {Convergence Rate of Frank-Wolfe for Non-Convex Objectives},
  year      = {2016},
  journal={arXiv:1607.00345}
}
@article{cohen-bary,
  author    = {Samuel Cohen and  Michael Arbel and Marc Peter Deisenroth},
  title     = {Estimating Barycenters of Measures in High Dimensions},
  year      = {2020},
  journal={arXiv:2007.07105}
}


@inproceedings{feydy2019interpolating,
    title={Interpolating Between Optimal Transport and MMD Using Sinkhorn Divergences},
    author={Feydy, Jean and S{\'e}journ{\'e}, Thibault and Vialard, Fran{\c{c}}ois-Xavier and Amari, Shun-ichi and Trouve, Alain and Peyr{\'e}, Gabriel},
    booktitle={AISTATS},
    year={2019}
}
@book{Kruskal:1978eu,
	author = {Kruskal, J. B.
and Wish, M.},
	title = {Multidimensional Scaling},
	publisher = {Sage Publications},
	year = {1978}
}
@incollection{NIPS2013_4927,
title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
author = {Cuturi, Marco},
booktitle = {NeurIPS},
year = {2013}
}






@book{alma991005863149705596,
  title={Optimal Transport: Old and New},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2008},
  publisher={Springer Science \& Business Media}
}


@inproceedings{DeisenrothEPF2014,
  title = {Multi-Task Policy Search for Robotics},
  author = {Deisenroth, MP. and Englert, P. and Peters, J. and Fox, D.},
  booktitle = {Proceedings of 2014 IEEE International Conference on Robotics and Automation},
  publisher = {IEEE},
  year = {2014}
}
@article{10.1016/j.patcog.2010.09.013,
author = {Petitjean, Fran\c{c}ois and Ketterlin, Alain and Gan\c{c}arski, Pierre},
title = {A Global Averaging Method for Dynamic Time Warping, with Applications to Clustering},
year = {2011},
journal = {Pattern Recognition},
keywords = {Global averaging, Satellite image time series, Sequence analysis, Time series averaging, Time series clustering, DTW barycenter averaging, Distance-based clustering, Dynamic time warping}
}


@Book{RePEc:mtp:titles:0262531925,
  author={Michael Carter},
  title={{Foundations of Mathematical Economics}},
  year=2001,
  series={MIT Press},
  edition={},
  keywords={mathematical economics; set theory},
  abstract={This book provides a comprehensive introduction to the mathematical foundations of economics, from basic set theory to fixed point theorems and constrained optimization. Rather than simply offer a collection of problem-solving techniques, the book emphasizes the unifying mathematical principles that underlie economics. Features include an extended presentation of separation theorems and their applications, an account of constraint qualification in constrained optimization, and an introduction to monotone comparative statics. These topics are developed by way of more than 800 exercises. The book is designed to be used as a graduate text, a resource for self-study, and a reference for the professional economist.}
}

@article{PETITJEAN201276,
title = "Summarizing a Set of Time Series by Averaging: From Steiner Sequence to Compact Multiple Alignment",
journal = "Theoretical Computer Science",
pages = {76--91},
volume = {414},

year = "2012",
author = "François Petitjean and Pierre Gançarski",
keywords = "Time series summarizing, Time series averaging, Dynamic time warping, Multiple alignment, Compact multiple alignment, Consensus sequence, Soft computing",
abstract = "Summarizing a set of sequences is an old topic that has been revived in the last decade, due to the increasing availability of sequential datasets. The definition of a consensus object is on the center of data analysis issues, since it crystallizes the underlying organization of the data. Dynamic Time Warping (DTW) is currently the most relevant similarity measure between sequences for a large panel of applications, since it makes it possible to capture temporal distortions. In this context, averaging a set of sequences is not a trivial task, since the average sequence has to be consistent with this similarity measure. The Steiner theory and several works in computational biology have pointed out the connection between multiple alignments and average sequences. Taking inspiration from these works, we introduce the notion of compact multiple alignment, which allows us to link these theories to the problem of summarizing under time warping. Having defined the link between the multiple alignment and the average sequence, the second part of this article focuses on the scan of the space of compact multiple alignments in order to provide an average sequence of a set of sequences. We propose to use a genetic algorithm based on a specific representation of the genotype inspired by genes. This representation of the genotype makes it possible to consistently paint the fitness landscape. Experiments carried out on standard datasets show that the proposed approach outperforms existing methods."
}

@article{10.1016/j.patcog.2008.11.030,
author = {Lemire, Daniel},
title = {Faster Retrieval with a Two-Pass Dynamic-Time-Warping Lower Bound},
year = {2009},
journal = {Pattern Recognition},
volume = {42},
pages = {2169-2180}
}


@article{10.1007/s10618-015-0418-x,
author = {Kate, Rohit J.},
title = {Using Dynamic Time Warping Distances as Features for Improved Time Series Classification},
year = {2016},
journal = {Data Mining Knowledge Discovery},
volume = {30},
number = {2},
pages = {283--312}
}

@article{Pass2014MultimarginalOT,
  title={Multi-Marginal Optimal Transport: Theory and Applications},
  author={Brendan Pass},
  journal={arXiv:1406.0026},
  year={2014}
}

@INPROCEEDINGS{655778,  author={ {Byoung-Kee Yi} and H. V. {Jagadish} and C. {Faloutsos}},  booktitle={ICDE},  title={Efficient Retrieval of Similar Time Sequences Under Time Warping},   year={1998}}



@InProceedings{pmlr-v70-cuturi17a,
  title = 	 {Soft-{DTW}: A Differentiable Loss Function for Time-Series},
  author = 	 {Marco Cuturi and Mathieu Blondel},
  booktitle = 	 {ICML},
  year = 	 {2017}
}


@article{Chapel2020PartialGW,
  title={Partial Gromov-Wasserstein with Applications on Positive-Unlabeled Learning},
  author={Laetitia Chapel and Mokhtar Z. Alaya and Gilles Gasso},
  journal={arXiv:2002.08276},
  year={2020}
}





@article{10.1016/j.patcog.2017.08.012,
author = {Schultz, David and Jain, Brijnesh},
title = {Nonsmooth Analysis and Subgradient Methods for Averaging in Dynamic Time Warping Spaces},
year = {2018},
journal = {Pattern Recognition},
volume = "74",
pages = "340--358",
keywords = {Subgradient methods, Time series averaging, Frchet function, Sample mean, Dynamic time warping}
}


@article{milgrom2002envelope,
  title={Envelope Theorems for Arbitrary Choice Sets},
  author={Milgrom, Paul and Segal, Ilya},
  volume = {70},
pages = {583-601},

  journal={Econometrica},
  year={2002},
  publisher={Wiley Online Library}
}

@inproceedings{fairbank2012value,
  title={Value-Gradient Learning},
  author={Fairbank, Michael and Alonso, Eduardo},
  booktitle={IJCNN},
  year={2012}
}

@inproceedings{heess2015learning,
  title={Learning Continuous Control Policies by Stochastic Value Gradients},
  author={Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
  booktitle={NeurIPS},
  year={2015}
}
@inproceedings{bishop2012bayesian,
  title={Bayesian hierarchical mixtures of experts},
  author={Bishop, Christopher M and Svens{\'e}n, Markus},
  year={2003},
  booktitle = {UAI}

}

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={ICML},
  year={2017}
}

@article{platt1999probabilistic,
  title={Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods},
  author={Platt, John},
  year={1999},
  journal={Advances in Large Margin Classifiers}
}